{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cfb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hzheng/miniconda3/envs/analysis-struct/lib/python3.12/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "# --- add (or ensure) these imports near the top of your file ---\n",
    "import re, shlex\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio import pairwise2\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part deal with downloading and preparing the pdb file\n",
    "def fetch_pdb(pdb_code, output_dir):\n",
    "    \"\"\"\n",
    "    Fetches a PDB file from RCSB and saves it locally.\n",
    "    \"\"\"\n",
    "    pdb_url = f\"https://files.rcsb.org/download/{pdb_code}.pdb\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdb_file = os.path.join(output_dir, f\"{pdb_code}.pdb\")\n",
    "    \n",
    "    response = requests.get(pdb_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(pdb_file, \"w\") as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Downloaded PDB file for {pdb_code} to {pdb_file}\")\n",
    "        return pdb_file\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not download PDB file for {pdb_code}.\")\n",
    "\n",
    "def download_prep(pdb_code, predicted_cif, output_dir, pdb_dir):\n",
    "    \"\"\"\n",
    "    Fetch the ground-truth PDB, then prep both that PDB and the predicted CIF\n",
    "    with your Schrodinger scripts. Returns paths to the two *_preped.pdb files.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(pdb_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Fetch ground-truth PDB\n",
    "    ground_truth_pdb = fetch_pdb(pdb_code, pdb_dir)\n",
    "\n",
    "    # 2) Build the prepped-output paths\n",
    "    truth_out = os.path.join(output_dir, f\"{pdb_code}_preped.pdb\")\n",
    "    pred_base = os.path.basename(predicted_cif).replace(\".cif\", \"_preped.pdb\")\n",
    "    pred_out = os.path.join(output_dir, pred_base)\n",
    "\n",
    "    # 3) Locate your shell‐wrapper scripts\n",
    "    script_pdb = \"./run_prep_single_pdb.sh\"\n",
    "    script_cif = \"./run_prep_single_cif.sh\"\n",
    "    for script in (script_pdb, script_cif):\n",
    "        if not os.path.isfile(script) or not os.access(script, os.X_OK):\n",
    "            raise FileNotFoundError(f\"Cannot execute wrapper script: {script}\")\n",
    "\n",
    "    # 4) Prepare ground-truth PDB\n",
    "    cmd1 = [script_pdb, ground_truth_pdb, output_dir]\n",
    "    cmd2 = [script_cif, predicted_cif, output_dir]\n",
    "    \n",
    "    try:\n",
    "        res1 = subprocess.run(\n",
    "            cmd1,\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"[prep pdb] {res1.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ prep failed for ground truth ({ground_truth_pdb}), exit {e.returncode}\")\n",
    "        print(e.stderr)\n",
    "        return None, None\n",
    "\n",
    "    # 5) Prepare predicted CIF\n",
    "    cmd2 = [script_cif, predicted_cif, output_dir]\n",
    "    try:\n",
    "        res2 = subprocess.run(\n",
    "            cmd2,\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"[prep cif] {res2.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ prep failed for predicted ({predicted_cif}), exit {e.returncode}\")\n",
    "        print(e.stderr)\n",
    "        return None, None\n",
    "\n",
    "    # 6) Return the two resulting PDB paths\n",
    "    return truth_out, pred_out\n",
    "\n",
    "def _worker_download_prep(args):\n",
    "    \"\"\"\n",
    "    Unpack arguments tuple and call download_prep.\n",
    "    Returns (pdb_code, truth_pdb, pred_pdb) or (pdb_code, None, None) on error.\n",
    "    \"\"\"\n",
    "    pdb_code, cif_path, output_dir, pdb_dir = args\n",
    "    try:\n",
    "        truth_pdb, pred_pdb = download_prep(pdb_code, cif_path, output_dir, pdb_dir)\n",
    "        return (pdb_code, truth_pdb, pred_pdb)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {pdb_code}: {e}\")\n",
    "        return (pdb_code, None, None)\n",
    "\n",
    "def multiprocess_directory_prep(\n",
    "    directory: str,\n",
    "    output_dir: str,\n",
    "    pdb_dir: str,\n",
    "    processes: int = 40\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each *_model.cif in `directory`, in parallel:\n",
    "      1. download ground-truth PDB\n",
    "      2. prep both ground-truth and predicted via download_prep()\n",
    "    Returns a pandas DataFrame with columns:\n",
    "      ['pdb_code', 'prepped_truth_pdb', 'prepped_predicted_pdb']\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # build list of jobs\n",
    "    jobs = []\n",
    "    suffix = '_model.cif'\n",
    "    for fn in os.listdir(directory):\n",
    "        if fn.lower().endswith(suffix):\n",
    "            pdb_code = fn[:-len(suffix)]\n",
    "            cif_path = os.path.join(directory, fn)\n",
    "            jobs.append((pdb_code, cif_path, output_dir, pdb_dir))\n",
    "\n",
    "    # launch Pool\n",
    "    with Pool(processes) as pool:\n",
    "        results = pool.map(_worker_download_prep, jobs)\n",
    "\n",
    "    # filter out failures and build DataFrame\n",
    "    successful = [\n",
    "        (code, truth, pred)\n",
    "        for code, truth, pred in results\n",
    "        if truth is not None and pred is not None\n",
    "    ]\n",
    "    df = pd.DataFrame(successful,\n",
    "                      columns=['pdb_code', 'prepped_truth_pdb', 'prepped_predicted_pdb'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896c2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_complex_prep_files(directory):\n",
    "    \"\"\"\n",
    "    In `directory`, find all files matching '*_model_prep.pdb' that contain '__'\n",
    "    and rename them so everything after the first '__' is dropped, e.g.:\n",
    "      1cbs__1__1.a__1.b_model_prep.pdb → 1cbs_model_prep.pdb\n",
    "    \"\"\"\n",
    "    base = Path(directory)\n",
    "    for f in base.glob(\"*_model_prep.pdb\"):\n",
    "        if \"__\" not in f.name:\n",
    "            continue\n",
    "        # f.stem is e.g. \"1cbs__1__1.a__1.b_model_prep\"\n",
    "        prefix = f.stem.split(\"__\", 1)[0]\n",
    "        new_name = f\"{prefix}_model_prep.pdb\"\n",
    "        target = f.with_name(new_name)\n",
    "        f.rename(target)\n",
    "        print(f\"Renamed: {f.name} → {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# optional: quiet the pairwise2 deprecation warning noise\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"Bio.pairwise2\")\n",
    "\n",
    "# ======================\n",
    "# poseviewer + analysis\n",
    "# ======================\n",
    "\n",
    "def _poseviewer_txt_name(pdb_path: str) -> str:\n",
    "    base, _ = os.path.splitext(os.path.basename(pdb_path))\n",
    "    return f\"{base}_pv_interactions.txt\"\n",
    "\n",
    "\n",
    "def _run_poseviewer(\n",
    "    pdb_path: str,\n",
    "    cwd: str,\n",
    "    force: bool = False,\n",
    "    sbgrid_rc: str = \"/programs/sbgrid.shrc\",\n",
    "    schrodinger_root: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run poseviewer_interactions.py on a PDB and return the *_pv_interactions.txt path.\n",
    "    If `schrodinger_root` is provided, use <root>/run directly.\n",
    "    Otherwise, source SBGrid (`sbgrid_rc`) to populate $SCHRODINGER, then run \"$SCHRODINGER/run\".\n",
    "    Output and logs are written in `cwd`.\n",
    "    \"\"\"\n",
    "    out_txt = os.path.join(cwd, _poseviewer_txt_name(pdb_path))\n",
    "    if os.path.exists(out_txt) and not force:\n",
    "        return out_txt\n",
    "\n",
    "    log_path = os.path.join(cwd, f\"{os.path.splitext(os.path.basename(pdb_path))[0]}_poseviewer.log\")\n",
    "\n",
    "    if schrodinger_root:\n",
    "        runner = os.path.join(schrodinger_root, \"run\")\n",
    "        if not os.path.exists(runner):\n",
    "            raise FileNotFoundError(f\"Schrödinger runner not found: {runner}\")\n",
    "        cmd = [runner, \"poseviewer_interactions.py\", pdb_path]\n",
    "        with open(log_path, \"w\") as log:\n",
    "            proc = subprocess.run(cmd, cwd=cwd, stdout=log, stderr=subprocess.STDOUT, check=False)\n",
    "    else:\n",
    "        # Use bash -lc so we can source the environment and then execute.\n",
    "        quoted_pdb = shlex.quote(pdb_path)\n",
    "        quoted_rc  = shlex.quote(sbgrid_rc)\n",
    "        bash_cmd = f'source {quoted_rc} >/dev/null 2>&1 && \"$SCHRODINGER/run\" poseviewer_interactions.py {quoted_pdb}'\n",
    "        with open(log_path, \"w\") as log:\n",
    "            proc = subprocess.run([\"bash\", \"-lc\", bash_cmd], cwd=cwd, stdout=log, stderr=subprocess.STDOUT, check=False)\n",
    "\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"poseviewer_interactions.py failed for {pdb_path} (see log: {log_path})\")\n",
    "    if not os.path.exists(out_txt):\n",
    "        raise FileNotFoundError(f\"Expected interactions file not found: {out_txt}\")\n",
    "    return out_txt\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# YOUR interaction extraction\n",
    "# ---------------------------\n",
    "\n",
    "def refined_extract_interaction_data_with_chain(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    interaction_sections = re.findall(\n",
    "        r\"Interactions grouped by receptor residue:\\n(.*?)Total \\w+ Contacts interactions:\",\n",
    "        content, re.S\n",
    "    )\n",
    "    interaction_type_list = []\n",
    "    residue_chain_list = []\n",
    "    residue_type_list = []\n",
    "    residue_number_list = []\n",
    "    contact_count_list = []\n",
    "    for section in interaction_sections:\n",
    "        lines = section.splitlines()\n",
    "        for line in lines:\n",
    "            match = re.match(r\"(\\w+)\\s+([A-Za-z0-9:]+)\\((\\w{3})\\)\\s+\\d+\\s+(\\d+)\", line.strip())\n",
    "            if match:\n",
    "                interaction_type, residue, amino_acid, contacts = match.groups()\n",
    "                chain_id = residue.split(\":\")[0]\n",
    "                residue_number = re.search(r\"(\\d+)\", residue).group()\n",
    "                interaction_type_list.append(interaction_type)\n",
    "                residue_chain_list.append(chain_id)\n",
    "                residue_type_list.append(amino_acid)\n",
    "                residue_number_list.append(int(residue_number))\n",
    "                contact_count_list.append(int(contacts))\n",
    "    df = pd.DataFrame({\n",
    "        \"Interaction Type\": interaction_type_list,\n",
    "        \"Residue Chain\": residue_chain_list,\n",
    "        \"Residue Type\": residue_type_list,\n",
    "        \"Residue Number\": residue_number_list,\n",
    "        \"# of Contacts\": contact_count_list\n",
    "    })\n",
    "    # drop \"Ugly\" and \"Bad\" if present\n",
    "    if not df.empty:\n",
    "        df = df[~df[\"Interaction Type\"].str.lower().isin([\"ugly\", \"bad\"])].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# YOUR sequence mapping funcs\n",
    "# ---------------------------\n",
    "\n",
    "def extract_sequence_and_residues(pdb_file, chain_id=None):\n",
    "    \"\"\"\n",
    "    Extracts the sequence and residue information from a PDB file,\n",
    "    handling breaks in chains by combining all peptides.\n",
    "    \"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"structure\", pdb_file)\n",
    "    model = structure[0]\n",
    "    ppb = PPBuilder()\n",
    "\n",
    "    chain_data = {}\n",
    "    for chain in model:\n",
    "        if chain_id and chain.id != chain_id:\n",
    "            continue\n",
    "\n",
    "        peptides = ppb.build_peptides(chain)\n",
    "        full_sequence = \"\"\n",
    "        full_residues = []\n",
    "\n",
    "        for peptide in peptides:\n",
    "            seq = str(peptide.get_sequence())\n",
    "            residues = [\n",
    "                (res.get_resname(), chain.id, res.id[1], res.id[2])  # (resname, chain, resnum, icode)\n",
    "                for res in peptide\n",
    "            ]\n",
    "            full_sequence += seq\n",
    "            full_residues.extend(residues)\n",
    "\n",
    "        if full_sequence:\n",
    "            chain_data[chain.id] = (full_sequence, full_residues)\n",
    "\n",
    "    return chain_data\n",
    "\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    alignment = pairwise2.align.globalxx(seq1, seq2)[0]\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def map_residues(alignment, residues1, residues2):\n",
    "    seq1, seq2 = alignment[0], alignment[1]\n",
    "    mapping = defaultdict(list)\n",
    "    idx1, idx2 = 0, 0\n",
    "\n",
    "    for char1, char2 in zip(seq1, seq2):\n",
    "        if char1 != \"-\":\n",
    "            idx1 += 1\n",
    "        if char2 != \"-\":\n",
    "            idx2 += 1\n",
    "        if char1 != \"-\" and char2 != \"-\":\n",
    "            pred_res = residues1[idx1 - 1]  # (resname, chain, resnum, icode)\n",
    "            true_res = residues2[idx2 - 1]\n",
    "            # key by (resnum, chain) on the predicted side, map to (resnum, chain) on the truth side\n",
    "            mapping[(pred_res[2], pred_res[1])].append((true_res[2], true_res[1]))\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def expand_predicted_interactions(predicted_interactions, all_mappings):\n",
    "    expanded_rows = []\n",
    "    for _, row in predicted_interactions.iterrows():\n",
    "        resnum = row[\"Residue Number\"]\n",
    "        chain = row[\"Residue Chain\"]\n",
    "        # Get all mappings for this residue\n",
    "        mapped_residues = all_mappings.get((resnum, chain), [])\n",
    "        for mapped_resnum, mapped_chain in mapped_residues:\n",
    "            expanded_row = row.copy()\n",
    "            expanded_row[\"Mapped Residue Number\"] = mapped_resnum\n",
    "            expanded_row[\"Mapped Chain\"] = mapped_chain\n",
    "            expanded_rows.append(expanded_row)\n",
    "\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    return expanded_df\n",
    "\n",
    "\n",
    "def analyze_interaction_recovery_with_expansion(predicted_file, true_file, predicted_pdb, true_pdb):\n",
    "    predicted_interactions = refined_extract_interaction_data_with_chain(predicted_file)\n",
    "    true_interactions = refined_extract_interaction_data_with_chain(true_file)\n",
    "\n",
    "    pred_seq_data = extract_sequence_and_residues(predicted_pdb)\n",
    "    true_data = extract_sequence_and_residues(true_pdb)\n",
    "\n",
    "    all_mappings = defaultdict(list)  # one-to-many mapping across chains\n",
    "    if isinstance(true_data, dict):\n",
    "        for _, (true_seq, true_reslist) in true_data.items():\n",
    "            for _, (pred_seq, pred_reslist) in pred_seq_data.items():\n",
    "                alignment = align_sequences(pred_seq, true_seq)\n",
    "                mapping = map_residues(alignment, pred_reslist, true_reslist)\n",
    "                for k, v in mapping.items():\n",
    "                    all_mappings[k].extend(v)\n",
    "    elif isinstance(true_data, tuple):\n",
    "        true_seq, true_reslist = true_data\n",
    "        for _, (pred_seq, pred_reslist) in pred_seq_data.items():\n",
    "            alignment = align_sequences(pred_seq, true_seq)\n",
    "            mapping = map_residues(alignment, pred_reslist, true_reslist)\n",
    "            for k, v in mapping.items():\n",
    "                all_mappings[k].extend(v)\n",
    "\n",
    "    # Expand predicted interactions with mappings\n",
    "    predicted_interactions = expand_predicted_interactions(predicted_interactions, all_mappings)\n",
    "\n",
    "    # Create Mapped Key for comparison\n",
    "    if predicted_interactions.empty:\n",
    "        recovery_percentage = 0.0\n",
    "        return recovery_percentage, true_interactions\n",
    "\n",
    "    predicted_interactions[\"Mapped Key\"] = (\n",
    "        predicted_interactions[\"Interaction Type\"].astype(str) + \"_\" +\n",
    "        predicted_interactions[\"Residue Type\"].astype(str) + \"_\" +\n",
    "        predicted_interactions[\"Mapped Residue Number\"].astype(str)\n",
    "    )\n",
    "\n",
    "    true_interactions[\"Key\"] = (\n",
    "        true_interactions[\"Interaction Type\"].astype(str) + \"_\" +\n",
    "        true_interactions[\"Residue Type\"].astype(str) + \"_\" +\n",
    "        true_interactions[\"Residue Number\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # Recovery check (require chain match too)\n",
    "    true_interactions[\"Recovered\"] = true_interactions.apply(\n",
    "        lambda row: any(\n",
    "            (row[\"Key\"] == mk and row[\"Residue Chain\"] == mc)\n",
    "            for mk, mc in zip(predicted_interactions[\"Mapped Key\"], predicted_interactions[\"Mapped Chain\"])\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    recovery_percentage = float(true_interactions[\"Recovered\"].mean() * 100.0) if not true_interactions.empty else 0.0\n",
    "    print(f\"Recovery Percentage: {recovery_percentage:.2f}%\")\n",
    "    return recovery_percentage, true_interactions\n",
    "\n",
    "\n",
    "# (optional) visualization helper you provided — left unchanged\n",
    "def visualize_mapping_details(mapping, predicted_reslist, true_reslist):\n",
    "    mapping_table = []\n",
    "    for pred_res, true_res_list in mapping.items():\n",
    "        for true_res in true_res_list:\n",
    "            mapping_table.append({\n",
    "                \"Predicted Residue Name\": pred_res[0],\n",
    "                \"Predicted Chain\": pred_res[1],\n",
    "                \"Predicted Residue Number\": pred_res[2],\n",
    "                \"True Residue Name\": true_res[0],\n",
    "                \"True Chain\": true_res[1],\n",
    "                \"True Residue Number\": true_res[2]\n",
    "            })\n",
    "    mapping_df = pd.DataFrame(mapping_table)\n",
    "    print(\"Residue Mapping Details:\")\n",
    "    print(mapping_df)\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "# ---------- pair + batch (now using YOUR analyzer) ----------\n",
    "\n",
    "def _analyze_pair_interactions(\n",
    "    pdb_id: str,\n",
    "    pair_dir: str,\n",
    "    force: bool = False,\n",
    "    sbgrid_rc: str = \"/programs/sbgrid.shrc\",\n",
    "    schrodinger_root: str | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    - runs poseviewer on aligned_ref_*.pdb and aligned_mob_*.pdb (with SBGrid env)\n",
    "    - computes interaction recovery via your analyze_interaction_recovery_with_expansion(...)\n",
    "    - writes <pdb_id>_true_interactions.csv (the parsed truth table)\n",
    "    \"\"\"\n",
    "    ref_pdb = os.path.join(pair_dir, f\"aligned_ref_{pdb_id}.pdb\")\n",
    "    mob_pdb = os.path.join(pair_dir, f\"aligned_mob_{pdb_id}.pdb\")\n",
    "\n",
    "    if not os.path.exists(ref_pdb):\n",
    "        return {\"pdb_id\": pdb_id, \"interaction_recovery_percent\": \"N/A\", \"status\": f\"missing {os.path.basename(ref_pdb)}\"}\n",
    "    if not os.path.exists(mob_pdb):\n",
    "        return {\"pdb_id\": pdb_id, \"interaction_recovery_percent\": \"N/A\", \"status\": f\"missing {os.path.basename(mob_pdb)}\"}\n",
    "\n",
    "    try:\n",
    "        true_txt = _run_poseviewer(ref_pdb, cwd=pair_dir, force=force, sbgrid_rc=sbgrid_rc, schrodinger_root=schrodinger_root)\n",
    "        pred_txt = _run_poseviewer(mob_pdb,  cwd=pair_dir, force=force, sbgrid_rc=sbgrid_rc, schrodinger_root=schrodinger_root)\n",
    "\n",
    "        # Use YOUR analyzer (sequence alignment -> residue mapping)\n",
    "        recovery_pct, true_interactions_df = analyze_interaction_recovery_with_expansion(\n",
    "            pred_txt, true_txt, mob_pdb, ref_pdb\n",
    "        )\n",
    "\n",
    "        out_csv = os.path.join(pair_dir, f\"{pdb_id}_true_interactions.csv\")\n",
    "        true_interactions_df.to_csv(out_csv, index=False)\n",
    "\n",
    "        return {\"pdb_id\": pdb_id, \"interaction_recovery_percent\": round(float(recovery_pct), 2), \"status\": \"ok\"}\n",
    "    except Exception as e:\n",
    "        return {\"pdb_id\": pdb_id, \"interaction_recovery_percent\": \"N/A\", \"status\": f\"error: {e}\"}\n",
    "\n",
    "\n",
    "def analyze_interactions_batch(\n",
    "    work_root: str,\n",
    "    out_csv: str,\n",
    "    workers: int = 4,\n",
    "    force: bool = False,\n",
    "    sbgrid_rc: str = \"/programs/sbgrid.shrc\",\n",
    "    schrodinger_root: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds work_root/<pdb_id>/aligned_ref_<pdb_id>.pdb and aligned_mob_<pdb_id>.pdb,\n",
    "    runs poseviewer + interaction recovery, writes CSV: pdb_id,interaction_recovery_percent,status\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for name in os.listdir(work_root):\n",
    "        d = os.path.join(work_root, name)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        if (os.path.exists(os.path.join(d, f\"aligned_ref_{name}.pdb\")) and\n",
    "            os.path.exists(os.path.join(d, f\"aligned_mob_{name}.pdb\"))):\n",
    "            candidates.append((name, d))\n",
    "    if not candidates:\n",
    "        raise RuntimeError(f\"No aligned pairs found under {work_root}\")\n",
    "\n",
    "    rows = []\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = [\n",
    "            ex.submit(_analyze_pair_interactions, pid, d, force, sbgrid_rc, schrodinger_root)\n",
    "            for (pid, d) in candidates\n",
    "        ]\n",
    "        for fut in as_completed(futs):\n",
    "            rows.append(fut.result())\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"pdb_id\", \"interaction_recovery_percent\", \"status\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved interaction recovery to {out_csv}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5971a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files_and_scores(source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Collect `_model.cif` files and extract the highest ranking score from `ranking_scores.csv`.\n",
    "    \n",
    "    Args:\n",
    "        source_dir (str): Path to the directory containing subdirectories with `_model.cif` files and `ranking_scores.csv`.\n",
    "        destination_dir (str): Path to the directory where `_model.cif` files will be copied.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with folder names and highest ranking scores.\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    scores_data = []  # To store folder names and highest scores\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        # Check for `_model.cif` and `ranking_scores.csv` in each subdirectory\n",
    "        cif_files = [f for f in files if f.endswith(\"_model.cif\")]\n",
    "        csv_files = [f for f in files if f == \"ranking_scores.csv\"]\n",
    "\n",
    "        if cif_files and csv_files:\n",
    "            # Copy `_model.cif` file\n",
    "            cif_file_path = os.path.join(root, cif_files[0])\n",
    "            destination_path = os.path.join(destination_dir, cif_files[0])\n",
    "            shutil.copy(cif_file_path, destination_path)\n",
    "\n",
    "            # Read the `ranking_scores.csv` file\n",
    "            csv_file_path = os.path.join(root, \"ranking_scores.csv\")\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Extract the highest ranking score\n",
    "            if \"ranking_score\" in df.columns:\n",
    "                highest_score = df[\"ranking_score\"].max()\n",
    "                scores_data.append({\"id\": os.path.basename(root)[0:4], \"Highest Ranking Score\": highest_score})\n",
    "    \n",
    "    # Create a DataFrame of scores\n",
    "    scores_df = pd.DataFrame(scores_data)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set up all the folder and copy some files\n",
    "source_dir = \"/path/to/AF3_Prediction_directory\"\n",
    "analysis_dir = \"/path/to/your/analysis/directory\"\n",
    "dest_dir = \"/path/to/predicted_files_directory\"\n",
    "work_dir = \"/path/to/work_dir\"\n",
    "scoring_path = \"/output/path/to/ranking_scores.csv\"\n",
    "pdb_dir = \"path/to/real/pdb_files\"\n",
    "analysis_result_csv_path = \"/output_path/analysis_results.csv\"\n",
    "interaction_csv_path = \"/output_path/interaction_recovery.csv\"\n",
    "\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.makedirs(pdb_dir, exist_ok=True)\n",
    "\n",
    "score_df = collect_files_and_scores(source_dir, dest_dir)\n",
    "score_df.to_csv(scoring_path, index=False)\n",
    "print(\"Collected files and scores, saved to:\", scoring_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv(scoring_path)\n",
    "for index, row in score_df.iterrows():\n",
    "    pdb_code = row['id']\n",
    "    try:\n",
    "        fetch_pdb(pdb_code, pdb_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch PDB for {pdb_code}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the PDB and CIF files using your Schrodinger scripts\n",
    "cmd1 = [\n",
    "        \"./run_prep_dir_cif.sh\",\n",
    "        dest_dir, work_dir\n",
    "    ]\n",
    "\n",
    "cmd2 = [\n",
    "         \"./run_prep_dir_pdb.sh\",\n",
    "        pdb_dir, work_dir\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a846ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    proc = subprocess.run(\n",
    "        cmd1,\n",
    "        check=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logging.error(f\"script failed: {e.stderr.strip()}\")\n",
    "    print(e.stderr.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5442c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    proc = subprocess.run(\n",
    "        cmd2,\n",
    "        check=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logging.error(f\"script failed: {e.stderr.strip()}\")\n",
    "    print(e.stderr.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b891c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the prepared files to only keep  pdb id\n",
    "rename_complex_prep_files(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04caca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis on the prepared files\n",
    "cmd = f\"source /programs/sbgrid.shrc && \\\n",
    "  $SCHRODINGER/run python3 align-batch-schrodinger-similarity.py \\\n",
    "    --real_dir {work_dir} \\\n",
    "    --pred_dir {work_dir} \\\n",
    "    --work_dir {work_dir} \\\n",
    "    --out_csv  {analysis_result_csv_path} \\\n",
    "    --workers  20\"\n",
    "subprocess.run(cmd, shell=True, check=True, executable='/bin/bash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interaction analysis using Schrodinger poseviwewer\n",
    "int_df = analyze_interactions_batch(\n",
    "    work_root=work_dir,\n",
    "    out_csv=interaction_csv_path,\n",
    "    workers=10,\n",
    "    force=False,\n",
    "    sbgrid_rc=\"/programs/sbgrid.shrc\",   # <<< this is the key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this section, we do the final processing of the results\n",
    "final_result_csv_path = \"/output_path/final_analysis_results.csv\"\n",
    "int_df = pd.read_csv(interaction_csv_path)\n",
    "analysis_result = pd.read_csv(analysis_result_csv_path)\n",
    "\n",
    "\n",
    "# Merge on pdb_code\n",
    "merged = analysis_result.merge(int_df, on=\"pdb_id\")\n",
    "\n",
    "# Keep only the required columns\n",
    "merged = merged[[\"pdb_id\", \"protein_ca_rmsd\", \"ligand_rmsd\", \"interaction_recovery_percent\"]]\n",
    "\n",
    "merged.dropna(inplace=True)  # Drop rows with NaN values\n",
    "\n",
    "\n",
    "# Save each metric to its own CSV in the same folder\n",
    "merged.to_csv(final_result_csv_path, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis-struct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
